# Behavioral Cloning Project

[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)

I completed this project as part of the Udacity's Self-Driving Car Engineer Nanodegree program.

![Video Output][video-Standard-gif]

[video-Standard-gif]: ./images/behavioral_cloning_sample.gif "Project Video GIF"
[image-Model-Architeture]: ./images/model_architecture.jpg "Model Architecture"
[image-Simulator]: ./images/data_collection.jpg "Simulator"
[image-original]: ./images/center-2017-02-06-16-20-04-855.jpg "Original Image"
[image-flipped]: ./images/center-2017-02-06-16-20-04-855-flipped.jpg "Flipped Image"

[image-befor-cropping]: ./images/original-image-before_cropping.jpg "Original Image"
[image-after-cropping]: ./images/cropped-image.jpg "Cropped Image"


Overview
---
In this project, we have used convolutional neural networks (CNNs) to map the raw pixels from a front-facing camera to the steering commands for a self-driving car. This deep-learning based model learns to emulate the behavior of human drivers and can be deployed as a self-driving car controller. The model is trained by the data collected in a simulator built by Udacity. It is trained on the road images captured by three cameras that are paired with the steering angels generated by a human driving a car in the simulator.

The model is never explicitly trained to detect specific features such as outline of the roads. Therefore, this approach eliminates the need for engineers to anticipate what it important in an image and to foresee all necessary control rules for driving.

The benefit of this approach is that the system can learn to steer with or without lane markings and it results in a smaller network model that optimizes all the processing needed for detection, planning, and control in a self-driving car.

The Project
---
The goals / steps of this project are the following:
* Use the Udacity simulator to collect data of good driving behavior
* Design, train and validate a model that predicts a steering angle from image data
* Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track.
* Summarize the results with a written report

### Simulator
Udacity has built a simulator where you can manullay steer a car around a track for data collection. The ca in the simulator can also be programmed to drive autonomously around the track.

This project makes use of this simulator to collect data and then train a deep-learning model to drive the car autonomously around the track.


### Files
Here are the important files :
* model.py (script used to create and train the model)
* drive.py (script to drive the car - feel free to modify this file)
* model.h5 (a trained Keras model)
* a report writeup file (either markdown or pdf)
* video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)


### Dependencies
## Details About Files In This Directory

### `drive.py`

Usage of `drive.py` requires you have saved the trained model as an h5 file, i.e. `model.h5`. Once the model has been saved, it can be used with drive.py using this command:

```sh
python drive.py model.h5
```

The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection.

Note: There is known local system's setting issue with replacing "," with "." when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add "export LANG=en_US.utf8" to the bashrc file.

#### Saving a video of the autonomous agent

```sh
python drive.py model.h5 run1
```

The fourth argument, `run1`, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten.

```sh
ls run1
```

The image file name is a timestamp of when the image was seen. This information is used by `video.py` to create a chronological video of the agent driving.

### `video.py`

```sh
python video.py run1
```

Creates a video based on images found in the `run1` directory. The name of the video will be the name of the directory followed by `'.mp4'`, so, in this case the video will be `run1.mp4`.

Optionally, one can specify the FPS (frames per second) of the video:

```sh
python video.py run1 --fps 48
```

Will run the video at 48 FPS. The default FPS is 60.

## Model Architecture and Training Strategy
### Solution Design
My first step was to try the [LeNet architecture](http://yann.lecun.com/exdb/lenet/). However, the model was not successful in keeping the car on the track. Then, I experimented with other architectures. The architecture by NVIDIA's Autonomous Car Group worked very well.

Designing the data collection and preprocessing procedure was crucial in feeding the model with the high quality training data.  

- Since driving in the track required steering to the left most of the time, the model had a tendency to steer the car to the left. To address this issue I augmented the data with the a track data that steered to the right.  
- The center camera by itself wasn't enough to keep the car in the center. I added the left and right camera images (with a correction factor on the angle) to help the car go back to the center.
- Some parts of the track had sharp turns. I needed to add more data for those challenging parts. I collected data in those parts by intentionally getting close to the outline of the road and then steering back to the center.


### Model Architecture
This is the architecture I used that is based on [NVDIA's end-to-end autonomous car system](https://devblogs.nvidia.com/deep-learning-self-driving-cars/).

![Image Model Architecture][image-Model-Architeture]

### Training Data

Training data was chosen to keep the vehicle driving on the road. Also, the data provided by Udacity, I used the first track and second track data. The simulator provides three different images: center, left and right cameras. Each image was used to train the model.

The README describes how the model was trained and what the characteristics of the dataset are. Information such as how the dataset was generated and examples of images from the dataset must be included.



##### Data Collection
I used the model is trained by the data collected in a simulator built by Udacity.

We feed the data collected from Simulator to our model, this data is fed in the form of images captured by 3 dashboard cams center, left and right. The output data contains a file data.csv which has the mappings of center, left and right images and the corresponding steering angle, throttle, brake and speed.

Using Keras Deep learning framework we can create a model.h5 file which we can test later on simulator with the command "python drive.py model.h5". This drive.py connects your model to simulator. The challenge in this project is to collect all sorts of training data so as to train the model to respond correctly in any type of situation.


I am using OpenCV to load the images, by default the images are read by OpenCV in BGR format but we need to convert to RGB as in drive.py it is processed in RGB format.
Since we have a steering angle associated with three images we introduce a correction factor for left and right images since the steering angle is captured by the center angle.
I decided to introduce a correction factor of 0.2
For the left images I increase the steering angle by 0.2 and for the right images I decrease the steering angle by 0.2
Sample Image

Here is a screen shot of the simulator during the data collection.
![Simulator][image-Simulator]

Recovery Laps

If you drive and record normal laps around the track, even if you record a lot of them, it might not be enough to train your model to drive properly.

Here’s the problem: if your training data is all focused on driving down the middle of the road, your model won’t ever learn what to do if it gets off to the side of the road. And probably when you run your model to predict steering measurements, things won’t go perfectly and the car will wander off to the side of the road at some point.

So you need to teach the car what to do when it’s off on the side of the road.

One approach might be to constantly wander off to the side of the road and then steer back to the middle.

A better approach is to only record data when the car is driving from the side of the road back toward the center line.

So as the human driver, you’re still weaving back and forth between the middle of the road and the shoulder, but you need to turn off data recording when you weave out to the side, and turn it back on when you steer back to the middle.

Collecting Enough Data
How do you know when you have collected enough data? Machine learning involves trying out ideas and testing them to see if they work. If the model is over or underfitting, then try to figure out why and adjust accordingly.

Since this model outputs a single continuous numeric value, one appropriate error metric would be mean squared error. If the mean squared error is high on both a training and validation set, the model is underfitting. If the mean squared error is low on a training set but high on a validation set, the model is overfitting. Collecting more data can help improve a model when the model is overfitting.

What if the model has a low mean squared error on both the training and validation sets, but the car is falling off the track?

Try to figure out the cases where the vehicle is falling off the track. Does it occur only on turns? Then maybe it's important to collect more turning data. The vehicle's driving behavior is only as good as the behavior of the driver who provided the data.

Here are some general guidelines for data collection:

- two or three laps of center lane driving
- one lap of recovery driving from the sides
- one lap focusing on driving smoothly around curves


##### Data Visualizations

##### Data Preprocessing
My first step was to try the LeNet](http://yann.lecun.com/exdb/lenet/) model with three epochs and the training data provided by Udacity. On the first track, the car went straight to the lake. I needed to do some pre-processing. A new Lambda layer was introduced to normalize the input images to zero means. This step allows the car to move a bit further, but it didn't get to the first turn. Another Cropping layer was introduced, and the first turn was almost there, but not quite.


I decided to shuffle the images so that the order in which images comes doesn't matters to the CNN
Augmenting the data- i decided to flip the image horizontally and adjust steering angle accordingly, I used cv2 to flip the images.
In augmenting after flipping multiply the steering angle by a factor of -1 to get the steering angle for the flipped image.
So according to this approach we were able to generate 6 images corresponding to one entry in .csv file


##### Data Augmentation
- Augmented the data by adding the same image flipped with a negative angle.

A effective technique for helping with the left turn bias involves flipping images and taking the opposite sign of the steering measurement. For example:

```Python
import numpy as np
image_flipped = np.fliplr(image)
measurement_flipped = -measurement
```

Image captured by the center camera:

![image-original]

Flipped image from the center camera:

![image-flipped]

##### Using Multiple Cameras
The simulator captures images from three cameras mounted on the car: a center, right and left camera. That’s because of the issue of recovering from being off-center.


For this project, recording recoveries from the sides of the road back to center is effective. But it is also possible to use all three camera images to train the model. When recording, the simulator will simultaneously save an image for the left, center and right cameras. Each row of the csv log file, driving_log.csv, contains the file path for each camera as well as information about the steering measurement, throttle, brake and speed of the vehicle.


During training, you want to feed the left and right camera images to your model as if they were coming from the center camera. This way, you can teach your model how to steer if the car drifts off to the left or the right.

Figuring out how much to add or subtract from the center angle will involve some experimentation.

During prediction (i.e. "autonomous mode"), you only need to predict with the center camera image.

It is not necessary to use the left and right images to derive a successful model. Recording recovery driving from the sides of the road is also effective.



##### Cropping images
The cameras in the simulator capture 160 pixel by 320 pixel images.

Not all of these pixels contain useful information, however. In the image above, the top portion of the image captures trees and hills and sky, and the bottom portion of the image captures the hood of the car.

Your model might train faster if you crop each image to focus on only the portion of the image that is useful for predicting a steering angle.

Keras provides the Cropping2D layer for image cropping within the model. This is relatively fast, because the model is parallelized on the GPU, so many images are cropped simultaneously.


```Python
model.add(Cropping2D(cropping=((50,20), (0,0)), input_shape=(160,320,3)))
```

Second step is to crop the image 70 pixels from top and 25 pixels from bottom. The image was cropped from top because I did not wanted to distract the model with trees and sky and 25 pixels from the bottom so as to remove the dashboard that is coming in the images.

Here is an image from the simulator:

![image-befor-cropping]

This is how the image looks after cropping:

![image-after-cropping]


##### Generators

### Model Architecture
The neural network uses convolution layers with appropriate filter sizes. Layers exist to introduce nonlinearity into the model. The data is normalized in the model.

The README provides sufficient details of the characteristics and qualities of the architecture, such as the type of model used, the number of layers, the size of each layer. Visualizations emphasizing particular qualities of the architecture are encouraged. Here is one such tool for visualization.

### Training and Validation Datasets
I analyzed the Udacity Dataset and found out that it contains 9 laps of track 1 with recovery data. I was satisfied with the data and decided to move on.
I decided to split the dataset into training and validation set using sklearn preprocessing library.
I decided to keep 15% of the data in Validation Set and remaining in Training Set
I am using generator to generate the data so as to avoid loading all the images in the memory and instead generate it at the run time in batches of 32. Even Augmented images are generated inside the generators.

### Visualizing Loss

### Reduce Overfitting
Train/validation/test splits have been used, and the model uses dropout layers or other methods to reduce overfitting.

### Model Parameters
Learning rate parameters are chosen with explanation, or an Adam optimizer is used.

The model used an Adam optimizer, so the learning rate was not tuned manually (model.py line 146).


## Simulation

### Recording Video

### Result
No tire may leave the drivable portion of the track surface. The car may not pop up onto ledges or roll over any surfaces that would otherwise be considered unsafe (if humans were in the vehicle).

[Click here to see the video of the final result](https://www.youtube.com/watch?v=VDgz93-pczQ&feature=youtu.be)

### Track Two
The simulator contains two tracks. To meet specifications, the car must successfully drive around track one. Track two is more difficult. See if you can get the car to stay on the road for track two as well.
