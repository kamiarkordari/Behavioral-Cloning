# Behavioral Cloning Project

[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)

I completed this project as part of the Udacity's Self-Driving Car Engineer Nanodegree program.

![Video Output][video-Standard-gif]

[video-Standard-gif]: ./images/behavioral_cloning_sample.gif "Project Video GIF"
[image-Model-Architeture]: ./images/model_architecture.jpg "Model Architecture"
[image-Simulator]: ./images/data_collection.jpg "Simulator"
[image-original]: ./images/center-2017-02-06-16-20-04-855.jpg "Original Image"
[image-flipped]: ./images/center-2017-02-06-16-20-04-855-flipped.jpg "Flipped Image"

[image-befor-cropping]: ./images/original-image-before_cropping.jpg "Original Image"
[image-after-cropping]: ./images/cropped-image.jpg "Cropped Image"
[image-final-result]: ./images/final_video_screenshot.jpg "Final Video Screenshot"

Overview
---
In this project, I used convolutional neural networks (CNNs) to map the raw pixels from a front-facing camera to the steering commands for a self-driving car. This deep-learning based model learns to emulate the behavior of human drivers and can be deployed as a self-driving car controller. I collected data in a simulator built by Udacity to train my model. The model is trained on the road images captured by three cameras that are paired with the steering angels generated by a human driving a car in the simulator.

The model is never explicitly trained to detect specific features such as outline of the roads. Therefore, this approach eliminates the need for engineers to anticipate what it important in an image and to foresee all necessary control rules for driving.

The Project
---
The goals / steps of this project are the following:
* Use the Udacity simulator to collect data of good driving behavior
* Design, train and validate a model that predicts a steering angle from image data
* Use the model to drive the vehicle autonomously around the track in the simulator. The vehicle should remain on the road for an entire loop around the track.
* Summarize the results with a written report

### Simulator
Udacity has built a simulator where you can manullay steer a car around a track for data collection. The car in the simulator can also be programmed to drive autonomously around the track.

This project makes use of this simulator to collect data and then train a deep-learning model to drive the car autonomously around the track.


### Files
Here are the main files in this repo:
* model.py (script used to create and train the model)
* drive.py (script to drive the car)
* model.h5 (a trained Keras model)
* video.mp4 (a video recording of the vehicle driving autonomously around the track)


### Details About Files

### `drive.py`

Usage of `drive.py` requires you have saved the trained model as an h5 file, i.e. `model.h5`. Once the model has been saved, it can be used with drive.py using this command:

```sh
python drive.py model.h5
```

The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection.

Note: There is known local system's setting issue with replacing "," with "." when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add "export LANG=en_US.utf8" to the bashrc file.

#### Saving a video of the autonomous agent

```sh
python drive.py model.h5 run1
```

The fourth argument, `run1`, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten.

```sh
ls run1
```

The image file name is a timestamp of when the image was seen. This information is used by `video.py` to create a chronological video of the agent driving.

### `video.py`

```sh
python video.py run1
```

Creates a video based on images found in the `run1` directory. The name of the video will be the name of the directory followed by `'.mp4'`, so, in this case the video will be `run1.mp4`.

Optionally, one can specify the FPS (frames per second) of the video:

```sh
python video.py run1 --fps 48
```

Will run the video at 48 FPS. The default FPS is 60.

### Solution Design
First I tried the [LeNet architecture](http://yann.lecun.com/exdb/lenet/). However, the model was not successful in keeping the car on the track. Then, I experimented with [NVDIA's end-to-end autonomous car architecture](https://devblogs.nvidia.com/deep-learning-self-driving-cars/). NVIDIA's model worked very well.

Designing the data collection and preparation procedure was crucial in feeding the model with the high quality training data. Here are some steps I took to enhance the training dataset:  

- Since driving in the track required steering to the left most of the time, the model had a tendency to steer the car to the left. To address this issue I augmented the data by flipping images to generate data that steered to the right.  
- The center camera by itself wasn't enough to keep the car in the center. I added the left and right camera images to help the car go back to the center.
- I used images from three camera on the car to help with recovering car to the center of the track.
- Driving in some parts of the track is challenging (for example, parts with sharp turns). I needed to add more data for those challenging parts. I collected data in those parts by intentionally getting close to the outline of the road and then steering back to the center.

### Model Architecture
The final neural network architecture uses five convolution layers followed by a flatten layer and four dense layers. Here is the summary of the layers of the architecture:

![Image Model Architecture][image-Model-Architeture]


##### Model Parameters
Adam optimizer is used as the optimizer. Since this model outputs a single continuous numeric value I picked mean squared error `mse` for error metric. I chose 3 epochs to stop the training early to avoid overfitting.

```python
model.compile(optimizer='adam', loss='mse')
model.fit(x_train, y_train, validation_split=0.2, shuffle=True, epochs=3)
```


##### Reduce Overfitting
I used cross-validation as a preventative measure against overfitting. If the mean squared error is high on both a training and validation set, the model is underfitting. If the mean squared error is low on a training set but high on a validation set, the model is overfitting.

To address overfitting I collected more data specially in challenging cases such as driving around the turns and steering back from shoulders to the center of the track. I used 3 epochs to train the model and to stop training early to reduce overfitting.


### Training/Validation Data
I used Udacity's simulator in the training mode to collect training data by driving the car on the track. I used both the data provided by Udacity and the data I collected myself for training.


##### Data Collection
The simulator generates images captured by 3 dashboard cams center, left and right. The `driving_log.csv` file provides the mappings of center, left and right images and the corresponding steering angle, throttle, brake and speed. For this project we only use the steering angle.

An important task in this project is to collect a comprehensive training dataset to train the model to respond correctly in any type of situation.

Driving and recording normal laps around the track, even if we record a lot of them, is not enough to train the model to drive properly.

Here’s the problem: if the training data is all focused on driving down the middle of the road, the model won’t ever learn what to do if it gets off to the side of the road.

So we need to teach the car what to do when it’s off on the side of the road.

To address this issue I recorded data by weaving back and forth between the middle of the road and the shoulder, and recording when steering back to the middle.

This seems to be a good guideline for data collection:
- two or three laps of center lane driving
- one lap of recovery driving from the sides
- one lap focusing on driving smoothly around curves


##### Data Preprocessing
We need to normalize images before using them for training. Lambda layer is a convenient way to parallelize image normalization. The lambda layer will also ensure that the model will normalize input images when making predictions in `drive.py`.

```python
model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(160,320,3)))
```

##### Data Augmentation
A effective technique for helping with the left turn bias involves flipping images and taking the opposite sign of the steering measurement.

```Python
import numpy as np
image_flipped = np.fliplr(image)
measurement_flipped = -measurement
```

By flipping images we will double the number of data in our training dataset.

Here is an image captured by the center camera:

![image-original]

And this is the same image after flipping:

![image-flipped]

##### Using Multiple Cameras
The simulator captures images from three cameras mounted on the car: a center, right and left camera.

We use all three images to recover from being off-center. When recording, the simulator will simultaneously save an image for the left, center and right cameras. Each row of the csv log file, `driving_log.csv`, contains the file path for each camera.

During training, we feed the left and right camera images to the model as if they were coming from the center camera. This way, we can teach the model how to steer if the car drifts off to the left or the right.

During prediction, in the 'autonomous mode', we only need to predict with the center camera image.

Figuring out how much to add or subtract from the center angle involves some experimentation. I used a correction factor of 0.2 (steering angle is normalized to be between -1 and 1) where I add 0.2 to the left images and subtract 0.2 for the right images.

```python
correction = 0.2
measurement = float(line[3])
measurements.append(measurement)
measurements.append(measurement+correction)
measurements.append(measurement-correction)
```


##### Cropping images
The cameras in the simulator capture images that are 160 x 320 pixels. Not all of these pixels contain useful information. For example the top portion of the image captures trees and hills and sky, and the bottom portion of the image captures the hood of the car.

The model trains faster if we crop each image to focus on only the portion of the image that is useful for predicting a steering angle.

Keras provides the Cropping2D layer for image cropping within the model. This is relatively fast, because the model is parallelized on the GPU, so many images are cropped simultaneously.


```Python
model.add(Cropping2D(cropping=((50,20), (0,0)), input_shape=(160,320,3)))
```
This layer crops out the 50 pixels from top and 20 pixels from bottom.


Here is an image from the simulator:

![image-befor-cropping]

This is how the image looks after cropping:

![image-after-cropping]


##### Validation Data
After shuffling the data, I split the dataset into training and validation set by keeping 20% of the data in validation Set.

### Result
I saved the trained model architecture as `model.h5` using `model.save('model.h5')`. Then, I test it in the simulator (in the autonomous mode) using `python drive.py model.h5` command.

Our goal is to train a model that can keep the car on the track and have no tire leave the drivable portion of the track. The car may not pop up onto ledges or roll over any surfaces that would otherwise be considered unsafe. And these goals are achieved as can be seen in the final video.

[![image-final-result]](https://www.youtube.com/watch?v=VDgz93-pczQ&feature=youtu.be)
Click on the above image to see the video of the final result.
