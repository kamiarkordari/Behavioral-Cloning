# Behavioral Cloning Project

[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)

I completed this project as part of the Udacity's Self-Driving Car Engineer Nanodegree program.

![Video Output][video-Standard-gif]

[video-Standard-gif]: ./images/behavioral_cloning_sample.gif "Project Video GIF"
[image-Model-Architeture]: ./images/model_architecture.jpg "Model Architecture"
[image-Simulator]: ./images/data_collection.jpg "Simulator"
[image-original]: ./images/center-2017-02-06-16-20-04-855.jpg "Original Image"
[image-flipped]: ./images/center-2017-02-06-16-20-04-855-flipped.jpg "Flipped Image"

[image-befor-cropping]: ./images/original-image-before_cropping.jpg "Original Image"
[image-after-cropping]: ./images/cropped-image.jpg "Cropped Image"


Overview
---
In this project, we have used convolutional neural networks (CNNs) to map the raw pixels from a front-facing camera to the steering commands for a self-driving car. This deep-learning based model learns to emulate the behavior of human drivers and can be deployed as a self-driving car controller. The model is trained by the data collected in a simulator built by Udacity. It is trained on the road images captured by three cameras that are paired with the steering angels generated by a human driving a car in the simulator.

The model is never explicitly trained to detect specific features such as outline of the roads. Therefore, this approach eliminates the need for engineers to anticipate what it important in an image and to foresee all necessary control rules for driving.

The benefit of this approach is that the system can learn to steer with or without lane markings and it results in a smaller network model that optimizes all the processing needed for detection, planning, and control in a self-driving car.

The Project
---
The goals / steps of this project are the following:
* Use the Udacity simulator to collect data of good driving behavior
* Design, train and validate a model that predicts a steering angle from image data
* Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track.
* Summarize the results with a written report

### Simulator
Udacity has built a simulator where you can manullay steer a car around a track for data collection. The ca in the simulator can also be programmed to drive autonomously around the track.

This project makes use of this simulator to collect data and then train a deep-learning model to drive the car autonomously around the track.


### Files
Here are the main files in this repo:
* model.py (script used to create and train the model)
* drive.py (script to drive the car)
* model.h5 (a trained Keras model)
* video.mp4 (a video recording of the vehicle driving autonomously around the track)


### Details About Files

### `drive.py`

Usage of `drive.py` requires you have saved the trained model as an h5 file, i.e. `model.h5`. Once the model has been saved, it can be used with drive.py using this command:

```sh
python drive.py model.h5
```

The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection.

Note: There is known local system's setting issue with replacing "," with "." when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add "export LANG=en_US.utf8" to the bashrc file.

#### Saving a video of the autonomous agent

```sh
python drive.py model.h5 run1
```

The fourth argument, `run1`, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten.

```sh
ls run1
```

The image file name is a timestamp of when the image was seen. This information is used by `video.py` to create a chronological video of the agent driving.

### `video.py`

```sh
python video.py run1
```

Creates a video based on images found in the `run1` directory. The name of the video will be the name of the directory followed by `'.mp4'`, so, in this case the video will be `run1.mp4`.

Optionally, one can specify the FPS (frames per second) of the video:

```sh
python video.py run1 --fps 48
```

Will run the video at 48 FPS. The default FPS is 60.

### Solution Design
First I tried the [LeNet architecture](http://yann.lecun.com/exdb/lenet/). However, the model was not successful in keeping the car on the track. Then, I experimented with [NVDIA's end-to-end autonomous car architecture](https://devblogs.nvidia.com/deep-learning-self-driving-cars/). NVIDIA's model worked very well.

Designing the data collection and preprocessing procedure was crucial in feeding the model with the high quality training data. Here are some steps I took to enhance the training dataset:  

- Since driving in the track required steering to the left most of the time, the model had a tendency to steer the car to the left. To address this issue I augmented the data by flipping images to generate data that steered to the right.  
- The center camera by itself wasn't enough to keep the car in the center. I added the left and right camera images to help the car go back to the center.
- Some parts of the track had sharp turns. I needed to add more data for those challenging parts. I collected data in those parts by intentionally getting close to the outline of the road and then steering back to the center.

### Model Architecture
The final neural network architecture uses five convolution layers to introduce nonlinearity into the model.

Here is the summary of the layers of the architecture:

![Image Model Architecture][image-Model-Architeture]


##### Model Parameters
Adam optimizer is used as the optimizer. Since this model outputs a single continuous numeric value I picked mean squared error `mse` for error metric.

```python
model.compile(optimizer='adam', loss='mse')
```


##### Reduce Overfitting
I used cross-validation as a preventative measure against overfitting. It is important to collect the right amount of data. The training data should include challenging cases such as driving around the turns and steering back from shoulders to the center of the track. Early stopping is another way to reduce overfitting. I used 3 epochs to train the model.

Here is a guideline to use for cross-validation: If the mean squared error is high on both a training and validation set, the model is underfitting. If the mean squared error is low on a training set but high on a validation set, the model is overfitting.

### Training/Validation Data
I used Udacity's simulator in the training mode to collect training data by driving the car on the track. I used both the data provided by Udacity and the data I collected myself.


##### Data Collection
The simulator generates images captured by 3 dashboard cams center, left and right. The `driving_log.csv` file provides the mappings of center, left and right images and the corresponding steering angle, throttle, brake and speed. For this project we only use the steering angle.

An important task in this project is to collect a comprehensive training dataset to train the model to respond correctly in any type of situation.

Driving and recording normal laps around the track, even if we record a lot of them, is not enough to train the model to drive properly.

Here’s the problem: if the training data is all focused on driving down the middle of the road, the model won’t ever learn what to do if it gets off to the side of the road.

So we need to teach the car what to do when it’s off on the side of the road.

To address this issue I recorded data by weaving back and forth between the middle of the road and the shoulder, and recording when steering back to the middle.

This seems to be a good guideline for data collection:
- two or three laps of center lane driving
- one lap of recovery driving from the sides
- one lap focusing on driving smoothly around curves


Here is a screen shot of the simulator during the data collection.
![Simulator][image-Simulator]





##### Data Preprocessing
The images need to be pre-processed before used for training. Here are the steps I took:
- used a Lambda layer to normalize the input images to zero means
- used cropping to remove the noisy parts of the image that don't provide any useful information
- Shuffle the images so that the order in which images comes doesn't impact the model

##### Data Augmentation
- Augmented the data by adding the same image flipped with a negative angle.

A effective technique for helping with the left turn bias involves flipping images and taking the opposite sign of the steering measurement. For example:

Since we have a steering angle associated with three images we introduce a correction factor for left and right images since the steering angle is captured by the center angle.
I decided to introduce a correction factor of 0.2
For the left images I increase the steering angle by 0.2 and for the right images I decrease the steering angle by 0.2

Augmenting the data- i decided to flip the image horizontally and adjust steering angle accordingly, I used cv2 to flip the images.
In augmenting after flipping multiply the steering angle by a factor of -1 to get the steering angle for the flipped image.
So according to this approach we were able to generate 6 images corresponding to one entry in .csv file


```Python
import numpy as np
image_flipped = np.fliplr(image)
measurement_flipped = -measurement
```

Image captured by the center camera:

![image-original]

Flipped image from the center camera:

![image-flipped]

##### Using Multiple Cameras
The simulator captures images from three cameras mounted on the car: a center, right and left camera. That’s because of the issue of recovering from being off-center.


For this project, recording recoveries from the sides of the road back to center is effective. But it is also possible to use all three camera images to train the model. When recording, the simulator will simultaneously save an image for the left, center and right cameras. Each row of the csv log file, driving_log.csv, contains the file path for each camera as well as information about the steering measurement, throttle, brake and speed of the vehicle.


During training, you want to feed the left and right camera images to your model as if they were coming from the center camera. This way, you can teach your model how to steer if the car drifts off to the left or the right.

Figuring out how much to add or subtract from the center angle will involve some experimentation.

During prediction (i.e. "autonomous mode"), you only need to predict with the center camera image.

It is not necessary to use the left and right images to derive a successful model. Recording recovery driving from the sides of the road is also effective.



##### Cropping images
The cameras in the simulator capture 160 pixel by 320 pixel images.

Not all of these pixels contain useful information, however. In the image above, the top portion of the image captures trees and hills and sky, and the bottom portion of the image captures the hood of the car.

Your model might train faster if you crop each image to focus on only the portion of the image that is useful for predicting a steering angle.

Keras provides the Cropping2D layer for image cropping within the model. This is relatively fast, because the model is parallelized on the GPU, so many images are cropped simultaneously.


```Python
model.add(Cropping2D(cropping=((50,20), (0,0)), input_shape=(160,320,3)))
```

Second step is to crop the image 70 pixels from top and 25 pixels from bottom. The image was cropped from top because I did not wanted to distract the model with trees and sky and 25 pixels from the bottom so as to remove the dashboard that is coming in the images.

Here is an image from the simulator:

![image-befor-cropping]

This is how the image looks after cropping:

![image-after-cropping]


##### Validation Data
I analyzed the Udacity Dataset and found out that it contains 9 laps of track 1 with recovery data. I was satisfied with the data and decided to move on.
I decided to split the dataset into training and validation set using sklearn preprocessing library.
I decided to keep 15% of the data in Validation Set and remaining in Training Set
I am using generator to generate the data so as to avoid loading all the images in the memory and instead generate it at the run time in batches of 32. Even Augmented images are generated inside the generators.

##### Generators


### Result
After the model is trained we save it as a file named `model.h5` that we can test in the simulator using `python drive.py model.h5` command. `drive.py` connects the model to simulator.


No tire may leave the drivable portion of the track surface. The car may not pop up onto ledges or roll over any surfaces that would otherwise be considered unsafe (if humans were in the vehicle).

##### Visualizing Loss

##### Final Video
[Click here to see the video of the final result](https://www.youtube.com/watch?v=VDgz93-pczQ&feature=youtu.be)
